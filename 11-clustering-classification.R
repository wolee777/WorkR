#
# 군집화( Clustering )와 분류( Classification )
#
# Machine Learning : 방대한 데이터를 컴퓨터가 스스로 분석하고 학습하여 유용한 정보를 얻어내거나
#                    미래를 예측하기 위한 예측모델을 만들어내는 기술
#                    사람의 음성을 이해해서 어떤 작업을 수행하거나 이미지 속에서 특정 물체를
#                    찾아내는 것도 머신러닝 기술에 속한다.
#
# 머신러닝 기술은 데이터 분석 분야에도 적용되는데, 
# 대표 기술중 하나가 군집화(clustering)와 분류(classification) 이다.
#
# 군집화와 분류의 개념
#
# 군집화( clustering )란 주어진 대상 데이터들을 유사성이 높은 것끼리 묶어주는 기술
#                     이러한 묶음을 군집(cluster), 범주(category), 그룹(group, class)등 다양한 용어 사용
#
#   어떤 데이터 A, B가 동일한 군집에 속해 있다는 것은 A, B의 성질이 비숫하다는 의미가 된다.
#   데이터 안에서 이러한 군집이 발견된다면 분석에 있어서 매우 중요한 의미를 갖는다.
#   그 이유는 개별 군집의 특성을 파악하고 군집과 군집의 차이를 분석하면 현실에 적용할 수 있는 
#   중요한 정보를 얻어낼 수 있기 때문이다. 
#   머신러닝에서는 데이터를 입력하면 군집을 자동으로 찾아준다.
#
# 분류( classification )란 그룹(group, class)의 형태로 알려진 데이터들이 있을 때 그룹을 모르는 어떤 
#                       데이터에 대해 어느 그룹에 속하는지를 예측하는 기술
#
#   이미 알려진 정상인 그룹과 환자 그룹이 있을 때 그룹 정보를 알 수 없는 어떤 데이터가 정상인에
#   속하는지 환자에 속하는지 예측하는 것이 분류의 문제이다.
#   분류 기술이 의료 분야에서 사용되면 질병 진단에 활용될 수 있고, 영상 처리에 적용되면 사진을 보고
#   남성인지 여성인지를 구분하는 문제에 적용될 수 있다.
#                       
#
# 군집화는 그룹 정보를 알 수 없는 데이터를 대상으로 그룹을 찾아주는 기술로 머신러닝에서
# 비지도학습(unsupervised learning)
#
# 분류는 이미 그룹 정보가 있는 데이터를 기반으로 그룹을 모르는 데이터의 그룹을 예측하는 기술로 
# 머신러닝에서는 지도학습(supervised learning)에 속한다.
#
# 군집화 알고리즘 : k-평균( k-means ) 알고리즘
# 분류 알고리즘 : k-최근접이웃( k-nearest neighbor ) 알고리즘
#
#
# k-평균( k-means ) 군집화 알고리즘
#
# k-평균 군집화 과정
#
# 군집의 개수 k가 2인 경우를 가정하여 주어진 데이터에서 2개의 군집을 찾는 과정
#
# 1단계 : 대상 데이터셋 준비. 이때 산점도 상의 점 하나가 관측값 하나를 의미
# 2단계 : 산점도 상의 임의의 점 2개를 만든다. 이 2개의 점은 나중에 군집이 완성되었을 때
#         각 군집의 중심점이 된다. 따라서 군집의 개수만큼 임의의 점을 생성한다.
# 3단계 : 산점도 상의 임의의 점들 하나하나와 임의의 점 2개와의 거리를 계산하여 두 점 중 
#         가까운 쪽으로 군집을 형성한다.
# 4단계 : 두 개의 군집에서 중심점을 다시 계산한다. 새로 계산한 중심점의 위치로 이동한다.
# 5단계 : 4단계의 과정을 반복한다.
# 6단계 : 더 이상 변동되지 않으면 군집의 중심점에 도달한 것으로 반복을 중단한다.
# 7단계 : 마지막으로 해당 점과 가까운점들을 해당 점의 군집으로 표시하고 군집화를 종료한다.
#
# k-평균 군집화에서는 점과 점 사이의 거리를 계산하는 과정이 있다. 점과 점 사이의 거리를
# 계산하는 다양한 방법이 있지만, 대개 유클리디안 거리( Euclidean distance )가 많이 사용된다.
# 
mydata <- iris[ , 1:4 ]

fit <- kmeans( x = mydata, centers = 3 )   # 군집화
fit
#
#K-means clustering with 3 clusters of sizes 33, 21, 96   -> 3개의 군집에 속한 데이터들의 개수
#
#Cluster means:
#  Sepal.Length Sepal.Width Petal.Length Petal.Width      -> 3개의 군집의 중심점 좌표
#1     5.175758    3.624242     1.472727   0.2727273
#2     4.738095    2.904762     1.790476   0.3523810
#3     6.314583    2.895833     4.973958   1.7031250
#
#Clustering vector:                                       -> 각 데이터에 데한 군집 번호
#  [1] 1 2 2 2 1 1 1 1 2 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 2 2 1 1 1 2 2 1 1 1 2 1 1 1 2 1 1 2 2 1 1 2 1 2 1 1 3 3 3 3 3 3 3 2 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3
#[75] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
#[149] 3 3
#
#Within cluster sum of squares by cluster:
#  [1]   6.432121  17.669524 118.651875
#(between_SS / total_SS =  79.0 %)
#
#Available components:
#  
#[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"    "size"         "iter"         "ifault"     
#

fit$cluster     # 각 데이터에 대한 군집 번호
fit$centers     # 각 군집의 중심점 좌표

# 차원 축소 후 군집 시각화
library( cluster )

#       군집대상  군집번호     원색깔표시     원빗금표시   관측값출력   중심점연결선표시
clusplot( mydata, fit$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0 )


# 데이터에서 두 번째 군집의 데이터만 추출
subset( mydata, fit$cluster == 2 )

# 대상 데이터 표준화 후 군집화
#   데이터와 데이터의 거리를 계산할 때 발생하는 문제
#   분석자들은 모든 변수가 거리 계산에 동등한 영향을 갖도록 하기 위해서 모든 변수의 
#   자료 범위를 0 ~ 1 사이로 표준화한 후에 거리 계산을 한다.
#   변수 A의 값들을 0 ~ 1 사이로 표준화하는 공식
#
#       ( x - min( A ) ) / ( max( A ) - min( A ) )
#         x는 변수 A의 임의의 관측값
#         max( A ), min( A )는 변수 A의 관측값 중 최대값과 최소값
#
# 표준화 함수
std <- function( x ) {
  return ( ( x - min( x ) ) / ( max( x ) - min( x ) ) ) 
}

mydata <- apply( iris[ , 1:4 ], 2, std )    # 표준화된 데이터 준비

fit <- kmeans( x = mydata, centers = 3 )
fit

#
# 군집화( clustering ) : 그룹 정보가 없는 데이터들 대상으로 그룹을 확인하는 기술
# 분류( classification ) : 그룹이 있는 데이터에 대해 그룹을 모르는 데이터가 들어왔을 때 
#                          어떤 그룹에 속하는지를 예측하는 기술
#
#
# k-최근접 이웃( KNN, K-Nearest Neighbor ) 알고리즘
# 
# k-최근접 이웃 분류 방법
#
# 1단계 : 그룹을 모르는 데이터 P에 대해 이미 그룹이 알려진 데이터 중 P와 가장 가까이에 있는 
#         k개의 데이터를 수집한다.
# 2단계 : k개의 데이터가 가장 많이 속해 있는 군집을 P의 군집으로 정한다.
#
# 다수결에 의해서 이웃의 점에 많은쪽 그룹으로 결정
#
library( class )

# 훈련용 데이터와 테스트용 데이터 준비
tr.idx <- c( 1: 25, 51:75, 101:125 )      # 훈련용 데이터의 인덱스

ds.tr <- iris[ tr.idx, 1:4 ]              # 훈련용 데이터셋
ds.ts <- iris[ -tr.idx, 1:4 ]             # 테스트용 데이터셋

cl.tr <- factor( iris[ tr.idx, 5 ] )      # 훈련용 데이터셋의 그룹(품종) 정보
cl.ts <- factor( iris[ -tr.idx, 5 ] )     # 테스트용 데이터셋의 그룹(품종) 정보

pred <- knn( ds.tr, ds.ts, cl.tr, k = 3, prob = TRUE )
#                                        prob : 예측된 그룹에 대한 지지 확률을 표시할지 여부 결정
#                                               k = 3인 경우 최근접 이웃이 class A가 2개, class B가 1개라면
#                                               예측된 그룹은 class A이고 class A에 대한 지지 확률은 2 / 3(=0.6666)이다.
#
# k값은 어떻게 정하는 가? : k값에 따라 예측모델의 정확도 향상. k값은 루트를 씌운 관측값의 개수보다는 작은 것이 좋다.
#                           예로 관측값 100이라면 k는 10보다 작은것이 좋다. 보통 1~7의 값을 차례로 실험해 보고 
#                                예측모델의 정확도가 높은 k 선택
#
# 그룹을 예측할 때 다수결에서 동수가 나오면 어떻게 하는가? : knn()에서는 의의로 선택
#
# k-최근점 이웃 알고리즘의 단점 : k개의 최근접 이웃을 알아내기 위해서는 그룹을 모르는 데이터 P와 그룹 정보가
#                                 알려진 훈련용 데이터셋의 모든 데이터들과으 거리 계산을 해야 한다.
#                                 거리 계산 시간이 많이 걸리고 모두 메모리에 있어야 함으로 메모리 요구량이 많다.
pred

acc <- mean( pred == cl.ts )    # 예측 정확도
acc

table( pred, cl.ts )            # 예측값과 실제값 비교 통계

#
# k-fold 교차 검증( k-fold cross validation )
#
# 머신러닝에서 예측모델을 개발하고 테스트하는 절차
#
# 1. 예측모델을 개발하려면 그룹 정보가 포함된 데이터셋 확보
# 2. 확보된 데이터셋은 훈련용 데이터( training data )와 테스트 데이터( test data )로 나눔
# 3. 훈련용 데이터를 이용하여 분류(예측) 모델을 개발
# 4. 테스트용 데이터를 이용하여 예측을 실시
# 5. 예측값과 실제값의 일치 정도를 계산하여 예측 정확도가 나오는데 이것이 모델의 성능을 나타낸다.
#
# 머신러닝의 목표 중 하나는 예측 정확도가 높은 모델을 만드는 것이다.
#
# 문제점 : 훈련용과 테스트용으로 데이터를 나눌 때 데이터가 어떻게 나누어졌는가에 따라 모델의 성능이 달라진다.
#
# 해결책 : 데이터를 임의로 훈련용과 테스트용으로 나누어 모델을 개발하는 과정을 여러번 반복하여 그곳에서 도출되는 
#          예측 정확도의 평균을 구하는 것 -> 체계화한 방법론이 k-fold cross validation이다.
#
# 교차 검증 방법
# 
# 1. 전체 데이터를 k등분한다.
# 2. k번의 실험에서 1 / k 등분의 데이터는 테스트용으로, 나머지는 훈련용으로 데이터를 나누어 모델을 개발하고,
#    정확도를 측정한 후 k번 측정한 정확도의 평균을 구한다.
install.packages( "cvTools" )
library( cvTools )      # cvFolds() 함수

k = 10        # 10-fold
folds <- cvFolds( nrow( iris ), K = k ) # 폴드 생성

acc <- c()    # 폴드별 예측 정확도 저장용 벡터
for ( i in 1:k ) {  
  ts.idx <- folds$which == i             # 테스트용 데이터의 인덱스
  ds.tr <- iris[ -ts.idx, 1:4 ]          # 훈련용 데이터셋
  ds.ts <- iris[ ts.idx, 1:4 ]           # 테스트용 데이터셋
  cl.tr <- factor( iris[ -ts.idx, 5 ] )  # 훈련용 데이터셋의 그룹(품종) 정보
  cl.ts <- factor( iris[ ts.idx, 5 ] )   # 테스트용 데이터셋의 그룹(품종) 정보
  
  pred <- knn( ds.tr, ds.ts, cl.tr, k = 5 )
  acc[ i ] <- mean( pred == cl.ts )      # 예측 정확도
}
acc                       # 폴드별 예측 정확도
mean( acc )               # 폴드평균 예측 정확도

